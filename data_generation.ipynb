{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a07942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from typing import Iterable, List, Union\n",
    "\n",
    "import multiprocess as mp\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from graph_state.graph_state import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics_for_parallelized_experiments(results):\n",
    "    print(\"\\n--- Experiment Run Summary ---\")\n",
    "    saved_count = sum(1 for r in results if r.startswith(\"Saved\"))\n",
    "    skipped_count = sum(1 for r in results if r.startswith(\"Skipped\"))\n",
    "    failed_count = sum(1 for r in results if r.startswith(\"Failed\"))\n",
    "    print(f\"Successfully saved: {saved_count}\")\n",
    "    print(f\"Skipped (existed):  {skipped_count}\")\n",
    "    print(f\"Failed:             {failed_count}\")\n",
    "    if failed_count > 0:\n",
    "        print(\"\\nFailures:\")\n",
    "        for r in results:\n",
    "            if r.startswith(\"Failed\"):\n",
    "                print(f\"  - {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_bell_sampling_worker(combination: tuple, num_repeats: int, output_dir: str, overwrite: bool) -> str:\n",
    "    \"\"\"\n",
    "    A single-process worker function for bell_sampling_fidelity_experiment.\n",
    "    'combination' is a tuple: (g: GraphState, err: error model (str), fidelity: float, shots: int, stab_factor: string indicating how many stabilizer elements we want to pick)\n",
    "    The process will run and save the data to the output_dir.\n",
    "\n",
    "    Returns:\n",
    "        A status string for logging.\n",
    "    \"\"\"\n",
    "    g, err, fid, shots, stab_factor = combination\n",
    "\n",
    "    if stab_factor == '2n':\n",
    "        numstab = g.n * 2\n",
    "    elif stab_factor == 'n^2':\n",
    "        numstab = g.n ** 2\n",
    "    elif stab_factor.isdigit():\n",
    "        numstab = int(stab_factor)\n",
    "    else:\n",
    "        return f\"Failed (param error): Stabilizer factor '{stab_factor}' not recognized\"\n",
    "\n",
    "    # Construct filename and check for overwrite\n",
    "    filename = f\"bell_fidelity_{g.n}q_F{fid:.3f}_err_{err}_shots_{shots}_numstab_{numstab}.npy\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    if not overwrite and os.path.exists(filepath):\n",
    "        return f\"Skipped (exists): {filename}\"\n",
    "    \n",
    "    # Run the actual experiment\n",
    "    try:\n",
    "        # all_fidelities = [fidelity_estimation_via_random_sampling_bitpacked(g, numstab, samples)]\n",
    "        all_fidelities = []\n",
    "        for seed_i in range(num_repeats):\n",
    "            samples = bell_sampling(g, err, fid, shots, seed=seed_i)\n",
    "            est_fidelity = fidelity_estimation_via_random_sampling_bitpacked(g, numstab, samples)\n",
    "            all_fidelities.append(est_fidelity)\n",
    "            \n",
    "        # Save the result\n",
    "        np.save(filepath, np.array(all_fidelities))\n",
    "        return f\"Saved ({len(all_fidelities)} repeats): {filename}\"\n",
    "    except Exception as e:\n",
    "        return f\"Failed (runtime error): {filename} with error: {e}\"\n",
    "\n",
    "def bell_sampling_fidelity_experiment(\n",
    "    graphs: Union[GraphState, List[GraphState]],\n",
    "    err_model: Union[str, List[str]],\n",
    "    fidelity: Union[float, Iterable[float]],\n",
    "    num_shots: Union[int, Iterable[int]],\n",
    "    num_repeats: int,\n",
    "    stabilizer_factors: Union[str, List[str]],\n",
    "    output_dir: str,\n",
    "    overwrite: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a Bell sampling fidelity experiment in parallel for all combinations of parameters. \n",
    "    Each combination is run in a separate process.\n",
    "    Arguments can be passed in as either a single instance or as a list and will be expanded.\n",
    "    \"\"\"\n",
    "    # Normalize all inputs to be lists\n",
    "    graphs_list = [graphs] if isinstance(graphs, GraphState) else list(graphs)\n",
    "    err_models = [err_model] if isinstance(err_model, str) else list(err_model)\n",
    "    fidelities = [fidelity] if isinstance(fidelity, (float, int)) else list(fidelity)\n",
    "    shots_list = [num_shots] if isinstance(num_shots, int) else list(num_shots)\n",
    "    stabilizer_factors_list = [stabilizer_factors] if isinstance(stabilizer_factors, str) else list(stabilizer_factors)\n",
    "\n",
    "    # Create the directory for saving results\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Saving experiment data to: '{output_dir}/'\")\n",
    "\n",
    "    # Create all combinations of the parameters\n",
    "    combinations = list(product(graphs_list, err_models, fidelities, shots_list, stabilizer_factors_list))\n",
    "    \n",
    "    print(f\"Starting Bell random sampling experiment for {len(graphs_list)} graph(s), \"\n",
    "          f\"{len(err_models)} error model(s), {len(fidelities)} fidelity value(s), \"\n",
    "          f\"{len(shots_list)} shot setting(s), and {len(stabilizer_factors_list)} stabilizer factor(s).\")\n",
    "    print(f\"Total combinations (jobs) to run: {len(combinations)}\")\n",
    "    \n",
    "    # Set up the partial function for the worker\n",
    "    worker_partial = partial(\n",
    "        _run_bell_sampling_worker,\n",
    "        num_repeats=num_repeats,\n",
    "        output_dir=output_dir,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "    \n",
    "    # Set up and run the multiprocessing pool\n",
    "    num_processes = max(1, mp.cpu_count() - 2) # Leave some cores free\n",
    "    print(f\"Running on {num_processes} processes...\")\n",
    "\n",
    "    results = []\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        for result in tqdm.tqdm(pool.imap_unordered(worker_partial, combinations), total=len(combinations), desc=\"Running Experiments\"):\n",
    "            results.append(result)\n",
    "\n",
    "    print_statistics_for_parallelized_experiments(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_tomo_worker(combination: tuple, g: GraphState, overlap_observables: bool, num_repeats: int, output_dir: str, overwrite: bool) -> str:\n",
    "    \"\"\"\n",
    "    A single-process worker function for partial_tomo_experiment.\n",
    "    'combination' is a tuple: (err, fid, total_shots)\n",
    "    \n",
    "    Returns:\n",
    "        A status string for logging.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        err, fid, total_shots = combination\n",
    "        N = 2 ** g.n\n",
    "\n",
    "        filename = f\"tomo_{g.n}q_F{fid:.3f}_err_{err}_shots_{total_shots}.npy\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        if not overwrite and os.path.exists(filepath):\n",
    "            return f\"Skipped (exists): {filename}\"\n",
    "\n",
    "        all_diags_for_combo = []\n",
    "        for seed_i in range(num_repeats):\n",
    "            exps = dge_combined(g, err, fid, total_shots, overlap_observables=overlap_observables, seed=seed_i)\n",
    "            exps = np.maximum(0, exps)\n",
    "            diags = get_diagonals_from_all_stabilizer_observables(g, exps)\n",
    "            all_diags_for_combo.append(diags)\n",
    "            \n",
    "        # Stack the results into a single 2D NumPy array\n",
    "        stacked_diags = np.array(all_diags_for_combo) # shape (num_repeats, number_of_diagonals)\n",
    "\n",
    "        # 5. Save the result\n",
    "        np.save(filepath, stacked_diags)\n",
    "        return f\"Saved ({stacked_diags.shape}): {filename}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Provide more context in the error message\n",
    "        return f\"Failed: {filename} with error: {e}\"\n",
    "\n",
    "def partial_tomo_experiment_parallelized(\n",
    "    g: GraphState,\n",
    "    err_model: Union[str, List[str]],\n",
    "    fidelity: Union[float, Iterable[float]],\n",
    "    total_shots: Union[int, Iterable[int]],\n",
    "    overlap_observables: bool,\n",
    "    num_repeats: int,\n",
    "    output_dir: str,\n",
    "    overwrite: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a partial tomography experiment in parallel for all combinations\n",
    "    of parameters. Each combination is run in a separate process.\n",
    "\n",
    "    Args:\n",
    "        g (GraphState): The graph state object.\n",
    "        err_model (Union[str, List[str]]): A single error model string or a list of them.\n",
    "        fidelity (Union[float, Iterable[float]]): A single fidelity value or an iterable.\n",
    "        total_shots (Union[int, Iterable[int]]): A single total_shot (count) value, \n",
    "                                                 or an iterable of values.\n",
    "        num_repeats (int): The number of times to repeat the experiment\n",
    "                           for each parameter combination.\n",
    "        output_dir (str): The directory where the output .npy files will be saved.\n",
    "        overwrite (bool): If False, skips the calculation if the output file\n",
    "                          already exists. If True, it will always run and overwrite\n",
    "                          any existing file.\n",
    "    \"\"\"\n",
    "    # Normalize all inputs to be lists\n",
    "    err_models = [err_model] if isinstance(err_model, str) else list(err_model)\n",
    "    fidelities = [fidelity] if isinstance(fidelity, (float, int)) else list(fidelity)\n",
    "    shots_list = [total_shots] if isinstance(total_shots, int) else list(total_shots)\n",
    "\n",
    "    # Create the directory for saving results\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Saving experiment data to: '{output_dir}/'\")\n",
    "\n",
    "    # Create all combinations of the parameters\n",
    "    combinations = list(product(err_models, fidelities, shots_list))\n",
    "    \n",
    "    print(f\"Starting tomography experiment for {g.n} qubits ({len(err_models)} error model(s), \"\n",
    "          f\"{len(fidelities)} fidelity value(s), and {len(shots_list)} shot setting(s).\")\n",
    "    print(f\"Total combinations (jobs) to run: {len(combinations)}\")\n",
    "    \n",
    "    # Set up the partial function for the worker\n",
    "    worker_partial = partial(\n",
    "        _run_tomo_worker,\n",
    "        g=g,\n",
    "        overlap_observables=overlap_observables,\n",
    "        num_repeats=num_repeats,\n",
    "        output_dir=output_dir,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "    \n",
    "    # Set up and run the multiprocessing pool\n",
    "    num_processes = max(1, mp.cpu_count() - 2) # Leave some cores free\n",
    "    print(f\"Running on {num_processes} processes...\")\n",
    "\n",
    "    results = []\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        for result in tqdm.tqdm(pool.imap_unordered(worker_partial, combinations), total=len(combinations), desc=\"Running Experiments\"):\n",
    "            results.append(result)\n",
    "\n",
    "    print_statistics_for_parallelized_experiments(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9754a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_bell_sampling_diagonal_worker(combination: tuple, g: GraphState, num_repeats: int, output_dir: str, overwrite: bool) -> str:\n",
    "    # Unpack the combination tuple\n",
    "    err, fid, shots = combination\n",
    "\n",
    "    # Construct filename and check for overwrite\n",
    "    filename = f\"bell_diag_{g.n}q_F{fid:.3f}_err_{err}_shots_{shots}.npy\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    if not overwrite and os.path.exists(filepath):\n",
    "        return f\"Skipped (exists): {filename}\"\n",
    "\n",
    "    actual_shots = (shots + 1) // 2\n",
    "    bell_samples = bell_sampling(g, err, fid, actual_shots * num_repeats, seed=num_repeats)\n",
    "    samples_split = np.split(bell_samples, num_repeats)\n",
    "\n",
    "    all_diags_for_combo = []\n",
    "\n",
    "    for samples in samples_split:\n",
    "        exps = np.array(\n",
    "            [\n",
    "                expectation_value_of_observables_from_bell_bitpacked(\n",
    "                    np.packbits(stab, bitorder=\"little\"), samples\n",
    "                )\n",
    "                for stab in g.generate_all_int_staiblizers()\n",
    "            ]\n",
    "        )\n",
    "        sqrt_exps_safe = np.sqrt(np.maximum(0, exps))\n",
    "        diags = get_diagonals_from_all_stabilizer_observables(g, sqrt_exps_safe)\n",
    "        all_diags_for_combo.append(diags)\n",
    "\n",
    "    stacked_diags = np.array(all_diags_for_combo) # Shape (num_repeats, number_of_diagonals)\n",
    "    np.save(filepath, stacked_diags)\n",
    "    return f\"Saved ({stacked_diags.shape}): {filename}\"\n",
    "\n",
    "def bell_sampling_diagonal_experiment(\n",
    "    g: GraphState,\n",
    "    err_model: Union[str, List[str]],\n",
    "    fidelity: Union[float, Iterable[float]],\n",
    "    num_shots: Union[int, Iterable[int]],\n",
    "    num_repeats: int,\n",
    "    output_dir: str,\n",
    "    overwrite: bool,\n",
    "):\n",
    "    err_models = [err_model] if isinstance(err_model, str) else list(err_model)\n",
    "    fidelities = [fidelity] if isinstance(fidelity, (float, int)) else list(fidelity)\n",
    "    shots_list = [num_shots] if isinstance(num_shots, int) else list(num_shots)\n",
    "\n",
    "    # Create the directory for saving results if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Saving experiment data to: '{output_dir}/'\")\n",
    "\n",
    "    # Create all combinations of the parameters\n",
    "    combinations = list(product(err_models, fidelities, shots_list))\n",
    "    \n",
    "    print(f\"Starting Bell diagonal experiment for {g.n} qubits, {len(err_models)} error model(s), {len(fidelities)} fidelity value(s), and {len(shots_list)} shot setting(s).\")\n",
    "    print(f\"Starting experiment for {len(combinations)} parameter combinations...\")\n",
    "    \n",
    "    # Set up the partial function for the worker\n",
    "    worker_partial = partial(\n",
    "        _run_bell_sampling_diagonal_worker,\n",
    "        num_repeats=num_repeats,\n",
    "        g=g,\n",
    "        output_dir=output_dir,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "    \n",
    "    # Set up and run the multiprocessing pool\n",
    "    num_processes = max(1, mp.cpu_count() - 2) # Leave some cores free\n",
    "    print(f\"Running on {num_processes} processes...\")\n",
    "\n",
    "    results = []\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        for result in tqdm.tqdm(pool.imap_unordered(worker_partial, combinations), total=len(combinations), desc=\"Running Experiments\"):\n",
    "            results.append(result)\n",
    "\n",
    "    print_statistics_for_parallelized_experiments(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_bell_sampling_diagonal_single_run_worker(combination: tuple[GraphState, str, float, int, int], output_dir: str, overwrite: bool) -> str:\n",
    "    g, err, fid, shots, repeat_idx = combination\n",
    "\n",
    "    # Construct filename and check for overwrite\n",
    "    filename = f\"bell_scalability_{g.n}q_F{fid:.3f}_err_{err}_shots_{shots}_repeat_{repeat_idx}.npy\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    if not overwrite and os.path.exists(filepath):\n",
    "        return f\"Skipped (exists): {filename}\"\n",
    "\n",
    "    actual_shots = (shots + 1) // 2\n",
    "    bell_samples = bell_sampling(g, err, fid, actual_shots, seed=1000 * g.n + repeat_idx)\n",
    "\n",
    "    exps = np.array(\n",
    "        [\n",
    "            expectation_value_of_observables_from_bell_bitpacked(\n",
    "                np.packbits(stab, bitorder=\"little\"), bell_samples\n",
    "            )\n",
    "            for stab in g.generate_all_int_staiblizers()\n",
    "        ]\n",
    "    )\n",
    "    sqrt_exps_safe = np.sqrt(np.maximum(0, exps))\n",
    "    diags = get_diagonals_from_all_stabilizer_observables(g, sqrt_exps_safe)\n",
    "\n",
    "    np.save(filepath, diags)\n",
    "    return f\"Saved ({diags.shape}): {filename}\"\n",
    "\n",
    "def bell_diagonal_scalability_experiment(\n",
    "    graphs: Union[GraphState, List[GraphState]],\n",
    "    err_model: Union[str, List[str]],\n",
    "    fidelity: Union[float, Iterable[float]],\n",
    "    num_shots: Union[int, Iterable[int]],\n",
    "    num_repeats: int,\n",
    "    output_dir: str,\n",
    "    overwrite: bool,\n",
    "):\n",
    "    # Normalize all inputs to be lists\n",
    "    graphs_list = [graphs] if isinstance(graphs, GraphState) else list(graphs)\n",
    "    err_models = [err_model] if isinstance(err_model, str) else list(err_model)\n",
    "    fidelities = [fidelity] if isinstance(fidelity, (float, int)) else list(fidelity)\n",
    "    shots_list = [num_shots] if isinstance(num_shots, int) else list(num_shots)\n",
    "    repeat_indices = [i for i in range(num_repeats)]\n",
    "\n",
    "    # Create the directory for saving results\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Saving experiment data to: '{output_dir}/'\")\n",
    "\n",
    "    # Create all combinations of the parameters\n",
    "    combinations = list(product(graphs_list, err_models, fidelities, shots_list, repeat_indices))\n",
    "    \n",
    "    print(f\"Starting Bell random sampling experiment for {len(graphs_list)} graph(s), \"\n",
    "          f\"{len(err_models)} error model(s), {len(fidelities)} fidelity value(s), \"\n",
    "          f\"{len(shots_list)} shot setting(s), and {num_repeats} trials.\")\n",
    "    print(f\"Total combinations (jobs) to run: {len(combinations)}\")\n",
    "    \n",
    "    # Set up the partial function for the worker\n",
    "    worker_partial = partial(\n",
    "        _run_bell_sampling_diagonal_single_run_worker,\n",
    "        output_dir=output_dir,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "    \n",
    "    # Set up and run the multiprocessing pool\n",
    "    num_processes = max(1, mp.cpu_count() - 2) # Leave some cores free\n",
    "    print(f\"Running on {num_processes} processes...\")\n",
    "\n",
    "    results = []\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        for result in tqdm.tqdm(pool.imap_unordered(worker_partial, combinations), total=len(combinations), desc=\"Running Experiments\"):\n",
    "            results.append(result)\n",
    "\n",
    "    print_statistics_for_parallelized_experiments(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75437091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We generate the data for 8 qubit graph here.\n",
    "    1. vary from F=0.5 to F=1 with N = 10^4\n",
    "    2. vary from 10^2 to 10^5 with F = 0.9 (plotting later)\n",
    "    X-axis will be log scale so we need to do log scaling X-axis (N)\n",
    "\"\"\"\n",
    "\n",
    "g = GraphState(8, \"complete\")\n",
    "shots = 10_000\n",
    "err_model = \"single-qubit-dephasing\"\n",
    "# err_model = \"depolarizing\"\n",
    "repeat = 1000\n",
    "OVERWRITE = False\n",
    "\n",
    "# first experiment\n",
    "partial_tomo_experiment_parallelized(\n",
    "    g,\n",
    "    err_model,\n",
    "    fidelity=0.9,\n",
    "    total_shots=np.round(np.logspace(start=2, stop=5, base=10, endpoint=True, num=50)).astype(int),\n",
    "    overlap_observables=False,\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-new/dephasing/dge_group_nonoverlapped\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "# second experiment\n",
    "partial_tomo_experiment_parallelized(\n",
    "    g,\n",
    "    err_model,\n",
    "    np.round(np.arange(0.5, 1.01, step=0.02), decimals=3),\n",
    "    shots,\n",
    "    overlap_observables=False,\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-new/dephasing/dge_group_nonoverlapped\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "# first experiment\n",
    "partial_tomo_experiment_parallelized(\n",
    "    g,\n",
    "    err_model,\n",
    "    fidelity=0.9,\n",
    "    total_shots=np.round(np.logspace(start=2, stop=5, base=10, endpoint=True, num=50)).astype(int),\n",
    "    overlap_observables=True,\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-new/dephasing/dge_group_overlapped\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "# second experiment\n",
    "partial_tomo_experiment_parallelized(\n",
    "    g,\n",
    "    err_model,\n",
    "    np.round(np.arange(0.5, 1.01, step=0.02), decimals=3),\n",
    "    shots,\n",
    "    overlap_observables=True,\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-new/dephasing/dge_group_overlapped\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "bell_sampling_diagonal_experiment(\n",
    "    g,\n",
    "    err_model,\n",
    "    fidelity=0.9,\n",
    "    num_shots=np.round(np.logspace(start=2, stop=5, base=10, endpoint=True, num=50)).astype(int),\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-new/dephasing/bsqn\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "bell_sampling_diagonal_experiment(\n",
    "    g,\n",
    "    err_model,\n",
    "    fidelity=np.round(np.arange(0.5, 1.01, step=0.02), decimals=3),\n",
    "    num_shots=shots,\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-new/dephasing/bsqn\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "# scalability experiment\n",
    "for n in range(2, 16):\n",
    "    g = GraphState(n, \"complete\")\n",
    "    partial_tomo_experiment_parallelized(\n",
    "        g,\n",
    "        'depolarizing',\n",
    "        0.9,\n",
    "        2 * 10_000,\n",
    "        overlap_observables=True,\n",
    "        num_repeats=25,\n",
    "        output_dir=\"data-new/scalability/dge_group_overlapped\",\n",
    "        overwrite=False,\n",
    "    )\n",
    "bell_diagonal_scalability_experiment(\n",
    "    [GraphState(n, \"complete\") for n in range(2, 24)],\n",
    "    'depolarizing',\n",
    "    fidelity=0.9,\n",
    "    num_shots=20_000,\n",
    "    num_repeats=25,\n",
    "    output_dir=\"data-new/scalability/bsqn\",\n",
    "    overwrite=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722912b1",
   "metadata": {},
   "source": [
    "## Now we process the raw data into CSV files keeping only the relevant information for plotting and archivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947ad701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_from_diags(diags_array: np.ndarray, true_diags: np.ndarray):\n",
    "    \"\"\"\n",
    "    Calculates various metrics from diagonal measurements for each repeat.\n",
    "    \n",
    "    Args:\n",
    "        diags_array: A 2D array of shape (num_repeats, num_diagonals).\n",
    "        true_diags: A 1D array of the true diagonal values.\n",
    "        input_fidelity: The fidelity value used as input for this experiment.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing three 1D arrays (one value per repeat):\n",
    "        - delta_a_norm_2: The L2 norm of the difference vector.\n",
    "        - estimated_fidelities: The calculated fidelity for each repeat.\n",
    "        - delta_fidelities: The difference between input and estimated fidelity.\n",
    "    \"\"\"\n",
    "\n",
    "    original_true_diags = np.array(true_diags)\n",
    "    true_diags = true_diags[np.newaxis, :]\n",
    "\n",
    "    # L2 norm of the difference vector (delta_a) for each repeat\n",
    "    delta_a_norm_2 = np.linalg.norm(diags_array - true_diags, axis=1)\n",
    "    delta_a_norm_1 = np.linalg.norm(diags_array - true_diags, ord=1, axis=1)\n",
    "    \n",
    "    estimated_fidelities = diags_array[:, 0]\n",
    "    delta_fidelities = np.abs(original_true_diags[0] - estimated_fidelities)\n",
    "    \n",
    "    return delta_a_norm_2, delta_fidelities, delta_a_norm_1\n",
    "\n",
    "def load_multiruns_diagonal_data(output_dir: str, file_prefix: str, df_prefix = None):\n",
    "    \"\"\"Loads all .npy files, calculates metrics, and returns pandas DataFrames.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    file_pattern = re.compile(\n",
    "        rf\"{file_prefix}_(\\d+)q_F([\\d.]+)_err_(.*?)_shots_(\\d+).npy\"\n",
    "    )\n",
    "\n",
    "    for filename in os.listdir(output_dir):\n",
    "        match = file_pattern.match(filename)\n",
    "        if match:\n",
    "            qubits, F_str, err, shots = match.groups()\n",
    "            qubits = int(qubits)\n",
    "            input_F = float(F_str)\n",
    "            \n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            diags_data = np.load(filepath)\n",
    "            true_diags = get_true_diagonals(qubits, input_F, err)\n",
    "            \n",
    "            # Calculate all metrics for the loaded data\n",
    "            delta_norms_2, delta_fidelities, delta_norms_1 = calculate_metrics_from_diags(diags_data, true_diags)\n",
    "\n",
    "            # testing the second element\n",
    "            estimated_second_element = diags_data[:, 1]\n",
    "            \n",
    "            # Delta b=1\n",
    "            delta_b_1 = np.abs(np.array([true_diags[1] for _ in estimated_second_element]) - estimated_second_element)\n",
    "            \n",
    "            # Append one record per repeat\n",
    "            for i in range(len(delta_norms_2)):\n",
    "                all_data.append({\n",
    "                    \"prefix\": file_prefix if df_prefix is None else df_prefix,\n",
    "                    \"qubits\": int(qubits),\n",
    "                    \"input_fidelity\": input_F,\n",
    "                    \"error_model\": err,\n",
    "                    \"total_shots\": int(shots),\n",
    "                    \"repeats\": i,\n",
    "                    \"diag_sanity\": np.sum(diags_data[i]), # sums the vector; it should sum to 1\n",
    "                    \"vector_a_norm_1\": np.linalg.norm(diags_data[i], ord=1),\n",
    "                    \"vector_a_norm_2\": np.linalg.norm(diags_data[i]),\n",
    "                    \"delta_a_norm_1\": delta_norms_1[i],\n",
    "                    \"delta_a_norm_2\": delta_norms_2[i],\n",
    "                    \"est_fidelity\": diags_data[i][0],\n",
    "                    \"delta_fidelity\": delta_fidelities[i],\n",
    "                    \"delta_b=1\": delta_b_1[i],\n",
    "                })\n",
    "                \n",
    "    if not all_data:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_scalability_data(output_dir: str, file_prefix: str, df_prefix: str = None):\n",
    "    \"\"\"Loads all .npy files, calculates metrics, and returns pandas DataFrames.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    file_pattern = re.compile(\n",
    "        rf\"{file_prefix}_(\\d+)q_F([\\d.]+)_err_(.*?)_shots_(\\d+)_repeat_(\\d+).npy\"\n",
    "    )\n",
    "\n",
    "    for filename in os.listdir(output_dir):\n",
    "        match = file_pattern.match(filename)\n",
    "        if match:\n",
    "            qubits, F_str, err, shots, repeat_idx = match.groups()\n",
    "            qubits = int(qubits)\n",
    "            input_F = float(F_str)\n",
    "            \n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            diags_data_single = np.array([np.load(filepath)])\n",
    "            true_diags = get_true_diagonals(qubits, input_F, err)\n",
    "            \n",
    "            # Calculate all metrics for the loaded data\n",
    "            delta_norms_2, delta_fidelities, delta_norms_1 = calculate_metrics_from_diags(diags_data_single, true_diags)\n",
    "\n",
    "            # testing the second element\n",
    "            estimated_second_element = diags_data_single[:, 1]\n",
    "            \n",
    "            # Delta b=1\n",
    "            delta_b_1 = np.abs(np.array([true_diags[1] for _ in estimated_second_element]) - estimated_second_element)\n",
    "            \n",
    "            # Append one record per repeat\n",
    "            all_data.append({\n",
    "                \"prefix\": file_prefix if df_prefix is None else df_prefix,\n",
    "                \"qubits\": int(qubits),\n",
    "                \"input_fidelity\": input_F,\n",
    "                \"error_model\": err,\n",
    "                \"total_shots\": int(shots),\n",
    "                \"repeats\": repeat_idx,\n",
    "                \"diag_sanity\": np.sum(diags_data_single), # sums the vector; it should sum to 1\n",
    "                \"vector_a_norm_1\": np.linalg.norm(diags_data_single, ord=1),\n",
    "                \"vector_a_norm_2\": np.linalg.norm(diags_data_single),\n",
    "                \"delta_a_norm_1\": delta_norms_1,\n",
    "                \"delta_a_norm_2\": delta_norms_2,\n",
    "                \"est_fidelity\": diags_data_single[0],\n",
    "                \"delta_fidelity\": delta_fidelities,\n",
    "                \"delta_b=1\": delta_b_1,\n",
    "            })\n",
    "                \n",
    "    if not all_data:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_bsqn_fidelity_estimation_data(output_dir: str):\n",
    "    \"\"\"\n",
    "    Loads all 'bell_fidelity' data, calculates metrics, and returns a DataFrame.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    file_pattern = re.compile(\n",
    "        r\"bell_fidelity_(\\d+)q_F([\\d.]+)_err_(.*?)_shots_(\\d+)_numstab_(\\d+).npy\"\n",
    "    )\n",
    "    \n",
    "    if not os.path.isdir(output_dir):\n",
    "        print(f\"Warning: Fidelity data directory '{output_dir}' not found. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(output_dir):\n",
    "        match = file_pattern.match(filename)\n",
    "        if match:\n",
    "            qubits, F_str, err, shots, numstab = match.groups()\n",
    "            qubits = int(qubits)\n",
    "            input_F = float(F_str)\n",
    "            shots = int(shots)\n",
    "            numstab = int(numstab)\n",
    "            \n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            fidelities_array = np.load(filepath)\n",
    "            \n",
    "            for i, est_fidelity in enumerate(fidelities_array):\n",
    "                all_data.append({\n",
    "                    \"qubits\": qubits,\n",
    "                    \"input_fidelity\": input_F,\n",
    "                    \"error_model\": err,\n",
    "                    \"shots\": shots,\n",
    "                    \"numstab\": numstab,\n",
    "                    \"repeat\": i,\n",
    "                    \"estimated_fidelity\": est_fidelity,\n",
    "                    \"fidelity_error\": np.abs(est_fidelity - input_F) # (est - true)\n",
    "                })\n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for experiment 1 and 2 for plotting for \n",
    "# \"Performance comparison between BSQN and DGE for an 8-node complete graph state\"\n",
    "\n",
    "dge_df = load_multiruns_diagonal_data('data-new/depolarizing/dge_group_overlapped', 'tomo', 'DGE')\n",
    "bsqn_df = load_multiruns_diagonal_data('data-new/depolarizing/bsqn', 'bell_diag', 'BSQN')\n",
    "output_dir = 'simulation_data/processed_csv'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "dge_df.to_csv(f'{output_dir}/performance_analysis_dge_overlapped.zip', encoding='utf-8', index=False)\n",
    "bsqn_df.to_csv(f'{output_dir}/performance_analysis_bsqn.zip', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb076c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for scalability experiment\n",
    "# \"Performance comparison between BSQN and DGE for an 2 to 200 nodes\"\n",
    "\n",
    "dge_df = load_multiruns_diagonal_data('data-new/scalability/dge_group_overlapped', 'tomo', 'DGE')\n",
    "bsqn_df = load_scalability_data('data-new/scalability/bsqn', 'bell_scalability', 'BSQN')\n",
    "output_dir = 'simulation_data/processed_csv'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "dge_df.to_csv(f'{output_dir}/scalability_analysis_dge_overlapped.zip', encoding='utf-8', index=False)\n",
    "bsqn_df.to_csv(f'{output_dir}/scalability_analysis_bsqn.zip', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "193fd1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for\n",
    "# Fidelity estimation data via random sampling with BSQN\n",
    "\n",
    "df = load_bsqn_fidelity_estimation_data('numerical_data/bell_fidelity_data')\n",
    "output_dir = 'simulation_data/processed_csv'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "df.to_csv(f'{output_dir}/random_sampling_fidelity_estimation_bsqn.zip', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bell-sampling-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
