{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52a07942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "import os\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from typing import Iterable, List, Union\n",
    "\n",
    "import tqdm\n",
    "from graph_state.graph_state import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d47578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics_for_parallelized_experiments(results):\n",
    "    print(\"\\n--- Experiment Run Summary ---\")\n",
    "    saved_count = sum(1 for r in results if r.startswith(\"Saved\"))\n",
    "    skipped_count = sum(1 for r in results if r.startswith(\"Skipped\"))\n",
    "    failed_count = sum(1 for r in results if r.startswith(\"Failed\"))\n",
    "    print(f\"Successfully saved: {saved_count}\")\n",
    "    print(f\"Skipped (existed):  {skipped_count}\")\n",
    "    print(f\"Failed:             {failed_count}\")\n",
    "    if failed_count > 0:\n",
    "        print(\"\\nFailures:\")\n",
    "        for r in results:\n",
    "            if r.startswith(\"Failed\"):\n",
    "                print(f\"  - {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_bell_sampling_worker(combination: tuple, num_repeats: int, output_dir: str, overwrite: bool) -> str:\n",
    "    \"\"\"\n",
    "    A single-process worker function for bell_sampling_fidelity_experiment.\n",
    "    'combination' is a tuple: (g: GraphState, err: error model (str), fidelity: float, shots: int, stab_factor: string indicating how many stabilizer elements we want to pick)\n",
    "    The process will run and save the data to the output_dir.\n",
    "\n",
    "    Returns:\n",
    "        A status string for logging.\n",
    "    \"\"\"\n",
    "    g, err, fid, shots, stab_factor = combination\n",
    "\n",
    "    if stab_factor == '2n':\n",
    "        numstab = g.n * 2\n",
    "    elif stab_factor == 'n^2':\n",
    "        numstab = g.n ** 2\n",
    "    elif stab_factor.isdigit():\n",
    "        numstab = int(stab_factor)\n",
    "    else:\n",
    "        return f\"Failed (param error): Stabilizer factor '{stab_factor}' not recognized\"\n",
    "\n",
    "    # Construct filename and check for overwrite\n",
    "    filename = f\"bell_fidelity_{g.n}q_F{fid:.3f}_err_{err}_shots_{shots}_numstab_{numstab}.npy\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    if not overwrite and os.path.exists(filepath):\n",
    "        return f\"Skipped (exists): {filename}\"\n",
    "    \n",
    "    # Run the actual experiment\n",
    "    try:\n",
    "        all_fidelities = [fidelity_estimation_via_random_sampling_bitpacked(g, numstab, samples)]\n",
    "        for _ in range(num_repeats):\n",
    "            samples = bell_sampling(g, err, fid, shots)\n",
    "            est_fidelity = fidelity_estimation_via_random_sampling_bitpacked(g, numstab, samples)\n",
    "            all_fidelities.append(est_fidelity)\n",
    "            \n",
    "        # Save the result\n",
    "        np.save(filepath, np.array(all_fidelities))\n",
    "        return f\"Saved ({len(all_fidelities)} repeats): {filename}\"\n",
    "    except Exception as e:\n",
    "        return f\"Failed (runtime error): {filename} with error: {e}\"\n",
    "\n",
    "def bell_sampling_fidelity_experiment(\n",
    "    graphs: Union[GraphState, List[GraphState]],\n",
    "    err_model: Union[str, List[str]],\n",
    "    fidelity: Union[float, Iterable[float]],\n",
    "    num_shots: Union[int, Iterable[int]],\n",
    "    num_repeats: int,\n",
    "    stabilizer_factors: Union[str, List[str]],\n",
    "    output_dir: str = \"bell_fidelity_data\",\n",
    "    overwrite: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a Bell sampling fidelity experiment in parallel for all combinations of parameters. \n",
    "    Each combination is run in a separate process.\n",
    "    Arguments can be passed in as either a single instance or as a list and will be expanded.\n",
    "    \"\"\"\n",
    "    # Normalize all inputs to be lists\n",
    "    graphs_list = [graphs] if isinstance(graphs, GraphState) else list(graphs)\n",
    "    err_models = [err_model] if isinstance(err_model, str) else list(err_model)\n",
    "    fidelities = [fidelity] if isinstance(fidelity, (float, int)) else list(fidelity)\n",
    "    shots_list = [num_shots] if isinstance(num_shots, int) else list(num_shots)\n",
    "    stabilizer_factors_list = [stabilizer_factors] if isinstance(stabilizer_factors, str) else list(stabilizer_factors)\n",
    "\n",
    "    # Create the directory for saving results\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Saving experiment data to: '{output_dir}/'\")\n",
    "\n",
    "    # Create all combinations of the parameters\n",
    "    combinations = list(product(graphs_list, err_models, fidelities, shots_list, stabilizer_factors_list))\n",
    "    \n",
    "    print(f\"Starting Bell random sampling experiment for {len(graphs_list)} graph(s), \"\n",
    "          f\"{len(err_models)} error model(s), {len(fidelities)} fidelity value(s), \"\n",
    "          f\"{len(shots_list)} shot setting(s), and {len(stabilizer_factors_list)} stabilizer factor(s).\")\n",
    "    print(f\"Total combinations (jobs) to run: {len(combinations)}\")\n",
    "    \n",
    "    # Set up the partial function for the worker\n",
    "    worker_partial = partial(\n",
    "        _run_bell_sampling_worker,\n",
    "        num_repeats=num_repeats,\n",
    "        output_dir=output_dir,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "    \n",
    "    # Set up and run the multiprocessing pool\n",
    "    num_processes = max(1, mp.cpu_count() - 2) # Leave some cores free\n",
    "    print(f\"Running on {num_processes} processes...\")\n",
    "\n",
    "    results = []\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        for result in tqdm.tqdm(pool.imap_unordered(worker_partial, combinations), total=len(combinations), desc=\"Running Experiments\"):\n",
    "            results.append(result)\n",
    "\n",
    "    print_statistics_for_parallelized_experiments(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_tomo_worker(combination: tuple, g: GraphState, overlap_observables: bool, num_repeats: int, output_dir: str, overwrite: bool) -> str:\n",
    "    \"\"\"\n",
    "    A single-process worker function for partial_tomo_experiment.\n",
    "    'combination' is a tuple: (err, fid, total_shots)\n",
    "    \n",
    "    Returns:\n",
    "        A status string for logging.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        err, fid, total_shots = combination\n",
    "        N = 2 ** g.n\n",
    "\n",
    "        filename = f\"tomo_{g.n}q_F{fid:.3f}_err_{err}_shots_{total_shots}.npy\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        if not overwrite and os.path.exists(filepath):\n",
    "            return f\"Skipped (exists): {filename}\"\n",
    "\n",
    "        all_diags_for_combo = []\n",
    "        for _ in range(num_repeats):\n",
    "            exps = dge_combined(g, err, fid, total_shots, overlap_observables=overlap_observables)\n",
    "            exps = np.maximum(0, exps)\n",
    "            diags = get_diagonals_from_all_stabilizer_observables(g, exps)\n",
    "            all_diags_for_combo.append(diags)\n",
    "            \n",
    "        # Stack the results into a single 2D NumPy array\n",
    "        stacked_diags = np.array(all_diags_for_combo) # shape (num_repeats, number_of_diagonals)\n",
    "\n",
    "        # 5. Save the result\n",
    "        np.save(filepath, stacked_diags)\n",
    "        return f\"Saved ({stacked_diags.shape}): {filename}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Provide more context in the error message\n",
    "        return f\"Failed: {filename} with error: {e}\"\n",
    "\n",
    "def partial_tomo_experiment_parallelized(\n",
    "    g: GraphState,\n",
    "    err_model: Union[str, List[str]],\n",
    "    fidelity: Union[float, Iterable[float]],\n",
    "    total_shots: Union[int, Iterable[int]],\n",
    "    overlap_observables: bool,\n",
    "    num_repeats: int,\n",
    "    output_dir: str,\n",
    "    overwrite: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a partial tomography experiment in parallel for all combinations\n",
    "    of parameters. Each combination is run in a separate process.\n",
    "\n",
    "    Args:\n",
    "        g (GraphState): The graph state object. (Note: This parallel version\n",
    "                        runs all jobs for a *single* graph state. The\n",
    "                        template supported a list of graphs, but this one\n",
    "                        follows the original function's signature.)\n",
    "        err_model (Union[str, List[str]]): A single error model string or a list of them.\n",
    "        fidelity (Union[float, Iterable[float]]): A single fidelity value or an iterable.\n",
    "        total_shots (Union[int, Iterable[int]]): A single *budget_shots*\n",
    "            value, or an iterable of values. Renamed from the original to\n",
    "            clarify it's the *budget* per combo.\n",
    "        num_repeats (int): The number of times to repeat the experiment\n",
    "                           for each parameter combination.\n",
    "        output_dir (str): The directory where the output .npy files will be saved.\n",
    "        overwrite (bool): If False, skips the calculation if the output file\n",
    "                          already exists. If True, it will always run and overwrite\n",
    "                          any existing file.\n",
    "    \"\"\"\n",
    "    # 1. Normalize all inputs to be lists\n",
    "    err_models = [err_model] if isinstance(err_model, str) else list(err_model)\n",
    "    fidelities = [fidelity] if isinstance(fidelity, (float, int)) else list(fidelity)\n",
    "    shots_list = [total_shots] if isinstance(total_shots, int) else list(total_shots)\n",
    "\n",
    "    # 2. Create the directory for saving results\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Saving experiment data to: '{output_dir}/'\")\n",
    "\n",
    "    # 3. Create all combinations of the parameters\n",
    "    combinations = list(product(err_models, fidelities, shots_list))\n",
    "    \n",
    "    print(f\"Starting tomography experiment for {g.n} qubits ({len(err_models)} error model(s), \"\n",
    "          f\"{len(fidelities)} fidelity value(s), and {len(shots_list)} shot setting(s).\")\n",
    "    print(f\"Total combinations (jobs) to run: {len(combinations)}\")\n",
    "    \n",
    "    # 4. Set up the partial function for the worker\n",
    "    worker_partial = partial(\n",
    "        _run_tomo_worker,\n",
    "        g=g,\n",
    "        overlap_observables=overlap_observables,\n",
    "        num_repeats=num_repeats,\n",
    "        output_dir=output_dir,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "    \n",
    "    # 5. Set up and run the multiprocessing pool\n",
    "    num_processes = max(1, mp.cpu_count() - 2) # Leave some cores free\n",
    "    print(f\"Running on {num_processes} processes...\")\n",
    "\n",
    "    results = []\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        # Use imap_unordered for efficiency, as order doesn't matter\n",
    "        # Wrap the iterator with tqdm to get a progress bar\n",
    "        for result in tqdm.tqdm(pool.imap_unordered(worker_partial, combinations), total=len(combinations), desc=\"Running Experiments\"):\n",
    "            results.append(result)\n",
    "            # Print results as they come in for better monitoring\n",
    "            # tqdm.tqdm.write(result) \n",
    "\n",
    "    print(\"\\n--- Experiment Run Summary ---\")\n",
    "    saved_count = sum(1 for r in results if r.startswith(\"Saved\"))\n",
    "    skipped_count = sum(1 for r in results if r.startswith(\"Skipped\"))\n",
    "    failed_count = sum(1 for r in results if r.startswith(\"Failed\"))\n",
    "    print(f\"Successfully saved: {saved_count}\")\n",
    "    print(f\"Skipped (existed):  {skipped_count}\")\n",
    "    print(f\"Failed:             {failed_count}\")\n",
    "    if failed_count > 0:\n",
    "        print(\"\\nFailures:\")\n",
    "        for r in results:\n",
    "            if r.startswith(\"Failed\"):\n",
    "                print(f\"  - {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9754a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_bell_sampling_diagonal_worker(combination: tuple, g: GraphState, num_repeats: int, output_dir: str, overwrite: bool) -> str:\n",
    "    # 1. Unpack the combination tuple\n",
    "    err, fid, shots = combination\n",
    "\n",
    "    # 2. Construct filename and check for overwrite\n",
    "    filename = f\"bell_diag_{g.n}q_F{fid:.3f}_err_{err}_shots_{shots}.npy\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    if not overwrite and os.path.exists(filepath):\n",
    "        return f\"Skipped (exists): {filename}\"\n",
    "\n",
    "    actual_shots = (shots + 1) // 2\n",
    "    bell_samples = bell_sampling(g, err, fid, actual_shots * num_repeats)\n",
    "    samples_split = np.split(bell_samples, num_repeats)\n",
    "    # return f\"Failed: {filename} with error: {samples_split[0].shape}\"\n",
    "\n",
    "    all_diags_for_combo = []\n",
    "\n",
    "    for samples in samples_split:\n",
    "        exps = np.array([expectation_value_of_observables_from_bell_bitpacked(np.packbits(stab, bitorder='little'), samples) for stab in g.generate_all_int_staiblizers()])\n",
    "        sqrt_exps_safe = np.sqrt(np.maximum(0, exps))\n",
    "        diags = get_diagonals_from_all_stabilizer_observables(g, sqrt_exps_safe)\n",
    "        all_diags_for_combo.append(diags)\n",
    "    \n",
    "    # Shape will be (num_repeats, number_of_diagonals)\n",
    "    stacked_diags = np.array(all_diags_for_combo)\n",
    "    np.save(filepath, stacked_diags)\n",
    "    return f\"Saved ({stacked_diags.shape}): {filename}\"\n",
    "\n",
    "def bell_sampling_diagonal_experiment(\n",
    "    g: GraphState,\n",
    "    err_model: Union[str, List[str]],\n",
    "    fidelity: Union[float, Iterable[float]],\n",
    "    num_shots: Union[int, Iterable[int]],\n",
    "    num_repeats: int,\n",
    "    output_dir: str,\n",
    "    overwrite: bool,\n",
    "):\n",
    "    err_models = [err_model] if isinstance(err_model, str) else list(err_model)\n",
    "    fidelities = [fidelity] if isinstance(fidelity, (float, int)) else list(fidelity)\n",
    "    shots_list = [num_shots] if isinstance(num_shots, int) else list(num_shots)\n",
    "\n",
    "    # 2. Create the directory for saving results if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Saving experiment data to: '{output_dir}/'\")\n",
    "\n",
    "    # 3. Create all combinations of the parameters\n",
    "    combinations = list(product(err_models, fidelities, shots_list))\n",
    "    \n",
    "    print(f\"Starting Bell diagonal experiment for {g.n} qubits, {len(err_models)} error model(s), {len(fidelities)} fidelity value(s), and {len(shots_list)} shot setting(s).\")\n",
    "    print(f\"Starting experiment for {len(combinations)} parameter combinations...\")\n",
    "    \n",
    "    # 4. Set up the partial function for the worker\n",
    "    worker_partial = partial(\n",
    "        _run_bell_sampling_diagonal_worker,\n",
    "        num_repeats=num_repeats,\n",
    "        g=g,\n",
    "        output_dir=output_dir,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "    \n",
    "    # 5. Set up and run the multiprocessing pool\n",
    "    num_processes = max(1, mp.cpu_count() - 2) # Leave some cores free\n",
    "    print(f\"Running on {num_processes} processes...\")\n",
    "\n",
    "    results = []\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        for result in tqdm.tqdm(pool.imap_unordered(worker_partial, combinations), total=len(combinations), desc=\"Running Experiments\"):\n",
    "            results.append(result)\n",
    "\n",
    "    print(\"\\n--- Experiment Run Summary ---\")\n",
    "    saved_count = sum(1 for r in results if r.startswith(\"Saved\"))\n",
    "    skipped_count = sum(1 for r in results if r.startswith(\"Skipped\"))\n",
    "    failed_count = sum(1 for r in results if r.startswith(\"Failed\"))\n",
    "    print(f\"Successfully saved: {saved_count}\")\n",
    "    print(f\"Skipped (existed):  {skipped_count}\")\n",
    "    print(f\"Failed:             {failed_count}\")\n",
    "    if failed_count > 0:\n",
    "        print(\"\\nFailures:\")\n",
    "        for r in results:\n",
    "            if r.startswith(\"Failed\"):\n",
    "                print(f\"  - {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75437091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving experiment data to: 'data-test/dephasing/dge_group_nonoverlapped/'\n",
      "Starting tomography experiment for 8 qubits (1 error model(s), 1 fidelity value(s), and 50 shot setting(s).\n",
      "Total combinations (jobs) to run: 50\n",
      "Running on 12 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Experiments: 100%|██████████| 50/50 [21:20<00:00, 25.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experiment Run Summary ---\n",
      "Successfully saved: 50\n",
      "Skipped (existed):  0\n",
      "Failed:             0\n",
      "Saving experiment data to: 'data-test/dephasing/dge_group_nonoverlapped/'\n",
      "Starting tomography experiment for 8 qubits (1 error model(s), 26 fidelity value(s), and 1 shot setting(s).\n",
      "Total combinations (jobs) to run: 26\n",
      "Running on 12 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Experiments: 100%|██████████| 26/26 [12:32<00:00, 28.94s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experiment Run Summary ---\n",
      "Successfully saved: 26\n",
      "Skipped (existed):  0\n",
      "Failed:             0\n",
      "Saving experiment data to: 'data-test/dephasing/dge_group_overlapped/'\n",
      "Starting tomography experiment for 8 qubits (1 error model(s), 1 fidelity value(s), and 50 shot setting(s).\n",
      "Total combinations (jobs) to run: 50\n",
      "Running on 12 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Experiments: 100%|██████████| 50/50 [20:48<00:00, 24.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experiment Run Summary ---\n",
      "Successfully saved: 50\n",
      "Skipped (existed):  0\n",
      "Failed:             0\n",
      "Saving experiment data to: 'data-test/dephasing/dge_group_overlapped/'\n",
      "Starting tomography experiment for 8 qubits (1 error model(s), 26 fidelity value(s), and 1 shot setting(s).\n",
      "Total combinations (jobs) to run: 26\n",
      "Running on 12 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Experiments: 100%|██████████| 26/26 [12:09<00:00, 28.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experiment Run Summary ---\n",
      "Successfully saved: 26\n",
      "Skipped (existed):  0\n",
      "Failed:             0\n",
      "Saving experiment data to: 'data-test/dephasing/bsqn/'\n",
      "Starting Bell diagonal experiment for 8 qubits, 1 error model(s), 1 fidelity value(s), and 50 shot setting(s).\n",
      "Starting experiment for 50 parameter combinations...\n",
      "Running on 12 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Experiments: 100%|██████████| 50/50 [00:00<00:00, 61500.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experiment Run Summary ---\n",
      "Successfully saved: 0\n",
      "Skipped (existed):  50\n",
      "Failed:             0\n",
      "Saving experiment data to: 'data-test/dephasing/bsqn/'\n",
      "Starting Bell diagonal experiment for 8 qubits, 1 error model(s), 26 fidelity value(s), and 1 shot setting(s).\n",
      "Starting experiment for 26 parameter combinations...\n",
      "Running on 12 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Experiments: 100%|██████████| 26/26 [00:00<00:00, 17726.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experiment Run Summary ---\n",
      "Successfully saved: 0\n",
      "Skipped (existed):  26\n",
      "Failed:             0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We generate the data for 8 qubit graph here.\n",
    "    1. vary from F=0.5 to F=1 with N = 10^4\n",
    "    2. vary from 10^2 to 10^5 with F = 0.9 (plotting later)\n",
    "    X-axis will be log scale so we need to do log scaling X-axis (N)\n",
    "\"\"\"\n",
    "\n",
    "g = GraphState(8, \"complete\")\n",
    "shots = 10_000\n",
    "err_model = \"single-qubit-dephasing\"\n",
    "repeat = 1000\n",
    "OVERWRITE = False\n",
    "\n",
    "# first experiment\n",
    "partial_tomo_experiment_parallelized(\n",
    "    g,\n",
    "    err_model,\n",
    "    fidelity=0.9,\n",
    "    total_shots=np.round(np.logspace(start=2, stop=5, base=10, endpoint=True, num=50)).astype(int),\n",
    "    overlap_observables=False,\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-appendix/dephasing/dge_group_nonoverlapped\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "# second experiment\n",
    "partial_tomo_experiment_parallelized(\n",
    "    g,\n",
    "    err_model,\n",
    "    np.round(np.arange(0.5, 1.01, step=0.02), decimals=3),\n",
    "    shots,\n",
    "    overlap_observables=False,\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-appendix/dephasing/dge_group_nonoverlapped\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "# first experiment\n",
    "partial_tomo_experiment_parallelized(\n",
    "    g,\n",
    "    err_model,\n",
    "    fidelity=0.9,\n",
    "    total_shots=np.round(np.logspace(start=2, stop=5, base=10, endpoint=True, num=50)).astype(int),\n",
    "    overlap_observables=True,\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-appendix/dephasing/dge_group_overlapped\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "# second experiment\n",
    "partial_tomo_experiment_parallelized(\n",
    "    g,\n",
    "    err_model,\n",
    "    np.round(np.arange(0.5, 1.01, step=0.02), decimals=3),\n",
    "    shots,\n",
    "    overlap_observables=True,\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-appendix/dephasing/dge_group_overlapped\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "bell_sampling_diagonal_experiment(\n",
    "    g,\n",
    "    err_model,\n",
    "    fidelity=0.9,\n",
    "    num_shots=np.round(np.logspace(start=2, stop=5, base=10, endpoint=True, num=50)).astype(int),\n",
    "    num_repeats=repeat,\n",
    "    output_dir=\"data-appendix/dephasing/bsqn\",\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "bell_sampling_diagonal_experiment(\n",
    "    g,\n",
    "    err_model,\n",
    "    np.round(np.arange(0.5, 1.01, step=0.02), decimals=3),\n",
    "    shots,\n",
    "    repeat,\n",
    "    \"data-appendix/dephasing/bsqn\",\n",
    "    False,\n",
    ")\n",
    "\n",
    "# # new scalability experiment\n",
    "\n",
    "# for n in range(2, 16):\n",
    "#     g = GraphState(n, \"complete\")\n",
    "#     partial_tomo_experiment_parallelized(\n",
    "#         g,\n",
    "#         err_model,\n",
    "#         0.9,\n",
    "#         2 * 10_000,\n",
    "#         overlap_observables=True,\n",
    "#         num_repeats=25,\n",
    "#         output_dir=\"data-scalability/dge_group_overlapped\",\n",
    "#         overwrite=False,\n",
    "#     )\n",
    "\n",
    "# # scalability experiment\n",
    "\n",
    "# for n in range(2, 13):\n",
    "#     g = GraphState(n, \"complete\")\n",
    "#     partial_tomo_experiment(\n",
    "#         g,\n",
    "#         err_model,\n",
    "#         0.9,\n",
    "#         2 * 10_000,\n",
    "#         25,\n",
    "#         \"tomo_data\",\n",
    "#         False,\n",
    "#     )\n",
    "#     bell_sampling_diagonal_experiment(\n",
    "#         g,\n",
    "#         err_model,\n",
    "#         0.9,\n",
    "#         20_000,\n",
    "#         25,\n",
    "#         \"bell_diag_data\",\n",
    "#         False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a2919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving experiment data to: 'bell_fidelity_data/'\n",
      "Starting Bell random sampling experiment for 1 graph(s), 3 error model(s), 1 fidelity value(s), 1 shot setting(s), and 50 stabilizer factor(s).\n",
      "Total combinations (jobs) to run: 150\n",
      "Running on 12 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Experiments: 100%|██████████| 150/150 [21:00:31<00:00, 504.21s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experiment Run Summary ---\n",
      "Successfully saved: 66\n",
      "Skipped (existed):  84\n",
      "Failed:             0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "graphs = [GraphState(i, 'complete') for i in range(21, 201)]\n",
    "shots = 5_000\n",
    "err_models = ['depolarizing', 'single-qubit-dephasing', 'bimodal']\n",
    "# err_models = ['depolarizing']\n",
    "stab_factors = '2n'\n",
    "repeat = 50\n",
    "OVERWRITE = False\n",
    "\n",
    "# testing scalability of fidelity estimation\n",
    "# bell_sampling_fidelity_experiment(graphs, err_models, 0.53, shots, repeat, stab_factors)\n",
    "\n",
    "# testing varying stabs\n",
    "num_qubits = 40\n",
    "shots = 1_000_000\n",
    "repeat = 50\n",
    "stabilizer_counts = np.logspace(np.log10(num_qubits), np.log10(num_qubits**3), 50).astype(int)\n",
    "stab_factors = list(map(str, stabilizer_counts))\n",
    "bell_sampling_fidelity_experiment(GraphState(40), err_models, 0.53, shots, repeat, stab_factors)\n",
    "\n",
    "# testing varying shots\n",
    "# num_qubits = 40\n",
    "# shots = np.logspace(2, 5, 50).astype(int)\n",
    "# stab_factors = '2n'\n",
    "# bell_sampling_fidelity_experiment(GraphState(40), err_models, 0.53, shots, repeat, stab_factors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bell-sampling-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
